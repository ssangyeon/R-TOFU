model_family: llama3-8b
model_path: deepseek-ai/DeepSeek-R1-Distill-Llama-8B


use_LoRA: false
LoRA:
  r: 8
  alpha: 32
  dropout: 0.05

forget_data: tofu
data_path: data/tofu
split: forget05
task_id: 1

forget_loss: GA+GD
lr: 1e-5
num_epochs: 5
batch_size: 4
gradient_accumulation_steps: 1
forget_coeff: 0.1 


beta: 0.1
weight_decay: 0.01

fix_ref_model: false
mask: true 

seed: 1001

save_checkpoint: false
overwrite_dir: false
save_steps: last 
save_root: results

save_dir: ${save_root}/${model_family}/${split}/${forget_loss}/seed_${seed}/epoch${num_epochs}_${lr}_FixRef${fix_ref_model}_mask${mask}_${forget_coeff}_${regularization_coeff}

ds_size: 300
eval_unlearn_step: last

eval:
  model_family: ${..model_family}
  forget_loss: ${..forget_loss}
  do_sample: false
  
  data_path: [ data/tofu, data/tofu, data/tofu, data/tofu ]

  split: ${..split}_perturbed
  split_list:
    - retain_perturbed
    - real_authors_perturbed
    - world_facts_perturbed
    - ${split}_perturbed

  eval_task: [ eval_log, eval_real_author_wo_options, eval_real_world_wo_options, eval_log_forget ]
  question_key: [ question, question, question, question ]
  answer_key: [ answer, answer, answer, answer ]
  base_answer_key: [ paraphrased_answer, answer, answer, paraphrased_answer ]
  perturbed_answer_key: [ perturbed_answer, perturbed_answer, perturbed_answer, perturbed_answer ]

  generation:
    max_length: 1024
    max_new_tokens: null

  save_generated_text: true
  
  ds_size: ${..ds_size}

  overwrite: true
  use_pretrained: false

  batch_size: 5